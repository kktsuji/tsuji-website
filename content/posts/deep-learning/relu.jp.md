---
title: "ReLU: Rectified Linear Unit"
description: ""
date: 2024-12-04T18:00:00+09:00
lastmod:
draft: false
math: true
---

$ ReLU(x) = max(0, x)$

ここで、$ x $はニューロンへの入力を示す。

バリアント：

- Leaky ReLU
- Parametric ReLU
- Constructed ReLU
- Gaussian-error Linear Unit (GeLU)
- など

参考文献：

- [Rectifier (neural networks) - wiki](<https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>)
