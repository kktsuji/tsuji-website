---
title: "LoRA vs. Fine-tuning"
description: ""
date: 2025-04-21T09:00:00+09:00
lastmod:
draft: false
---

Generated by Google Gemini.

## Traditional Fine-tuning

- Involves updating all or a large portion of the pre-trained model's weights during training on the new dataset.
- Aims to adapt the entire model's knowledge to the target task.
- Can be more computationally expensive and require more data.

## LoRA (Low-Rank Adaptation)

- Modifies only a small set of parameters (rank decomposition matrices) within specific layers.
- Focuses on adapting the model's behavior by injecting new information, rather than overwriting existing knowledge.
- More efficient in terms of memory and training time.

## So, is it appropriate to call it "fine-tuning"?

- In a broad sense, yes. LoRA is a technique for adapting a pre-trained model to a new task, which aligns with the general goal of fine-tuning.
- However, it's more precise to refer to it as "LoRA fine-tuning" or "fine-tuning with LoRA" to distinguish it from traditional full fine-tuning. This clarifies that you're using a specific method that differs from updating all weights.

## Ultimately, the terminology can be flexible, but providing context is key

- If you're discussing LoRA among researchers or practitioners familiar with the technique, simply saying "fine-tuning" might be understood.
- In other contexts, it's best to be more specific and explicitly mention LoRA to avoid ambiguity and ensure clear communication.
