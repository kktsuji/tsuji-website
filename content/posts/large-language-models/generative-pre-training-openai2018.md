---
title: "Improving Language Understanding by Generative Pre-Training"
description: ""
date: 2024-09-04T07:00:00+09:00
lastmod:
draft: false
---

Title: Improving Language Understanding by Generative Pre-Training

Authors: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever

Published: Jun 11, 2018

Link: [https://openai.com/index/language-unsupervised/](https://openai.com/index/language-unsupervised/)

Summary (Generated by Microsoft Copilot):

**Introduction:**

- The paper explores a semi-supervised approach for natural language understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning.

**Challenges:**

- The scarcity of labeled data for specific tasks.
- Difficulty in leveraging linguistic information from unlabeled data.
- Uncertainty in optimization objectives and effective transfer methods.

**Methods:**

- Two-stage training: unsupervised pre-training on a large corpus of text followed by supervised fine-tuning on specific tasks.
- Use of the Transformer model for better handling long-term dependencies.

**Novelties:**

- Task-aware input transformations during fine-tuning.
- Minimal changes to the model architecture for effective transfer.

**Results:**

- Significant improvements in 9 out of 12 natural language understanding tasks.
- Notable performance gains in commonsense reasoning, question answering, and textual entailment.

**Performances:**

- Achieved state-of-the-art results on various benchmarks, including an 8.9% improvement on the Stories Cloze Test and 5.7% on RACE.

**Limitations:**

- Performance on smaller datasets like RTE was lower compared to larger datasets.

**Discussion:**

- The approach demonstrates the effectiveness of generative pre-training and discriminative fine-tuning in improving natural language understanding tasks.
