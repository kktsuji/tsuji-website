---
title: "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"
description: ""
date: 2024-09-11T17:30:00+09:00
lastmod:
draft: false
---

Title: BioBERT: a pre-trained biomedical language representation model for biomedical text mining

Authors: Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang

Published: Jan 25 2019

Link: [https://arxiv.org/abs/1901.08746](https://arxiv.org/abs/1901.08746)

Summary (Generated by Microsoft Copilot):

**Introduction:**

- 生物医学文献の急速な成長により、生物医学テキストマイニングは極めて重要である。BioBERTは、生物医学テキストマイニングのためにBERTを適応させるために導入された。

**Challenges:**

- 一般的なNLPモデルは、異なる単語分布のため、生物医学テキストでパフォーマンスが低い。

**Methods:**

- BioBERTは大規模な生物医学コーパスで事前学習され、NER、RE、QAなどの特定のタスクでファインチューニングされる。

**Novelties:**

- BioBERTは、生物医学ドメインのために特別に事前学習された最初のBERTベースモデルである。

**Results:**

- BioBERTは、生物医学NER、RE、QAタスクにおいてBERTや他のモデルを大幅に上回る。

**Performances:**

- 最先端モデルと比較して、様々なデータセットでより高いF1とMRRスコアを達成している。

**Limitations:**

- 計算集約的な事前学習プロセス。

**Discussion:**

- 生物医学コーパスでの事前学習は、効果的な生物医学テキストマイニングに不可欠である。将来のバージョンには、ドメイン固有の語彙が含まれる予定である。

BioBERTは、PubMedを含む大規模な生物医学コーパスで事前学習されている。具体的には、モデルは以下で学習された:

1. **PubMed Abstracts**: このデータセットには、PubMed要約からの約45億語が含まれている。

2. **PMC Full-Text Articles**: このデータセットには、PubMed Central（PMC）の全文記事からの約135億語が含まれている。

これらの広範なデータセットは、BioBERTが生物医学テキストをより効果的に理解し処理するのに役立ち、生物医学ドメインにおける固有表現認識、関係抽出、質問応答などのタスクに非常に適したものにしている。
