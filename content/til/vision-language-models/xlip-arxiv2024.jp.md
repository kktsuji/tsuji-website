---
title: "XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training"
description: ""
date: 2024-09-22T18:00:00+09:00
lastmod:
draft: false
---

Title: XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training

Authors: Biao Wu, Yutong Xie, Zeyu Zhang, Minh Hieu Phan, Qi Chen, Ling Chen, Qi Wu

Published: Jul 28 2024

Link: [https://arxiv.org/abs/2407.19546](https://arxiv.org/abs/2407.19546)

Summary (Generated by Microsoft Copilot):

**Introduction:**

- この論文は、クロスモーダルアテンションマスクモデリングを使用した**医療言語画像事前学習**のためのフレームワークである**XLIP**を紹介する。

**Challenges:**

- 病理学的特徴の正確な再構成のための**医療データの不足**。
- 既存の手法は、ペアデータまたはペアでないデータのみを使用することが多く、両方を使用しない。

**Methods:**

- 特徴学習を強化するための**attention-masked image modeling（AttMIM）**と**entity-driven masked language modeling（EntMLM）**。
- 疾患種別プロンプトを使用して、ペアデータとペアでないデータの両方を活用する。

**Novelties:**

- より良い病理学的特徴学習のための**クロスモーダルアテンションマスキング**。
- アテンションガイドとプロンプト駆動のマスキングを統合した**ブレンディングマスキング戦略**。

**Results:**

- 5つのデータセットにおけるzero-shotおよびファインチューニング分類で**state-of-the-art（SOTA）**パフォーマンスを達成する。

**Performances:**

- 様々な医療画像分類タスクにおいて、MedKLIP、GLORIA、CheXzeroなどの既存モデルを上回る。

**Limitations:**

- この論文では明示的に制限事項について言及していないが、データの不足とモデルの複雑さにおける課題が示唆されている。

**Discussion:**

- 提案されたXLIPフレームワークは、医療データの表現と分類において大幅な改善を示し、高度な医療VLP手法の可能性を強調している。
