---
title: 'MedCLIP: Contrastive Learning from Unpaired Medical Images and Text'
description: ''
date: 2024-09-01T16:30:00+09:00
lastmod: 
draft: false
---

Title: MedCLIP: Contrastive Learning from Unpaired Medical Images and Text

Authors: Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, Jimeng Sun

Link: [https://arxiv.org/abs/2210.10163](https://arxiv.org/abs/2210.10163)

Published: Oct. 18, 2022

Summary (Generated by Microsoft Copilot):

**Introduction:**
- MedCLIP addresses the challenge of limited paired medical image-text datasets by decoupling images and texts for multimodal contrastive learning.

**Challenges:**
- **Data Insufficiency:** Medical image-text datasets are much smaller than general datasets.
- **False Negatives:** Separate patient images and reports with similar semantics are wrongly treated as negatives.

**Methods:**
- **Decoupling Images and Texts:** Utilizes unpaired datasets to scale training data.
- **Semantic Matching Loss:** Uses medical knowledge to eliminate false negatives.

**Novelties:**
- Combines unpaired images and texts to expand training data.
- Introduces a semantic matching loss based on medical knowledge.

**Results:**
- Outperforms state-of-the-art methods in zero-shot prediction, supervised classification, and image-text retrieval.

**Performances:**
- Achieves superior accuracy with significantly less pre-training data compared to other methods.

**Limitations:**
- Challenges with incorrect semantic tags and missing detection of negation or uncertainty phrases.

**Discussion:**
- MedCLIP demonstrates high data efficiency and transferability to various downstream tasks, supporting foundational models for medical diagnosis.
