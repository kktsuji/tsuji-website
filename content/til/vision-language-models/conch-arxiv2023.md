---
title: 'Towards a Visual-Language Foundation Model for Computational Pathology'
description: ''
date: 2024-08-30T8:30:00+09:00
lastmod: 
math: false
draft: false
---

Title: Towards a Visual-Language Foundation Model for Computational Pathology

Authors: Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Andrew Zhang, Long Phi Le, Georg Gerber, Anil V Parwani, Faisal Mahmood

Link: [https://arxiv.org/abs/2307.12914](https://arxiv.org/abs/2307.12914)

Summary (Generated by Microsoft Copilot):

**Introduction:**
- The paper introduces CONCH, a visual-language foundation model for histopathology, developed using over 1.17 million image-caption pairs.

**Challenges:**
- Label scarcity in medical domains and the labor-intensive process of data collection and annotation.

**Methods:**
- Utilizes contrastive learning and captioning objectives for pretraining, leveraging diverse histopathology images and biomedical text.

**Novelties:**
- CONCH's ability to perform zero-shot classification, segmentation, and retrieval tasks without task-specific training data.

**Results:**
- Achieves state-of-the-art performance across 13 diverse benchmarks, including histology image classification and segmentation.

**Performances:**
- Outperforms other models like PLIP and BiomedCLIP in various tasks, often by a significant margin.

**Limitations:**
- The scale of pretraining data is smaller compared to general visual-language models, limiting zero-shot recognition capabilities.

**Discussion:**
- Highlights the potential of CONCH to reduce the burden of annotating training examples and improve retrieval capabilities in clinical settings.
