---
title: 'Improving Language Understanding by Generative Pre-Training'
description: ''
date: 2024-09-04T07:00:00+09:00
lastmod: 
math: false
draft: false
---

Title: Improving Language Understanding by Generative Pre-Training

Authors: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever

Published: Jun 11, 2018

Link: [https://openai.com/index/language-unsupervised/](https://openai.com/index/language-unsupervised/)

Summary (Generated by Microsoft Copilot):

**Introduction:**
- The paper explores a semi-supervised approach for natural language understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning.

**Challenges:**
- The scarcity of labeled data for specific tasks.
- Difficulty in leveraging linguistic information from unlabeled data.
- Uncertainty in optimization objectives and effective transfer methods.

**Methods:**
- Two-stage training: unsupervised pre-training on a large corpus of text followed by supervised fine-tuning on specific tasks.
- Use of the Transformer model for better handling long-term dependencies.

**Novelties:**
- Task-aware input transformations during fine-tuning.
- Minimal changes to the model architecture for effective transfer.

**Results:**
- Significant improvements in 9 out of 12 natural language understanding tasks.
- Notable performance gains in commonsense reasoning, question answering, and textual entailment.

**Performances:**
- Achieved state-of-the-art results on various benchmarks, including an 8.9% improvement on the Stories Cloze Test and 5.7% on RACE.

**Limitations:**
- Performance on smaller datasets like RTE was lower compared to larger datasets.

**Discussion:**
- The approach demonstrates the effectiveness of generative pre-training and discriminative fine-tuning in improving natural language understanding tasks.
