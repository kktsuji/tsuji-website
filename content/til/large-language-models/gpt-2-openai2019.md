---
title: 'Language Models are Unsupervised Multitask Learners'
description: ''
date: 2024-09-03T21:30:00+09:00
lastmod: 
draft: false
---

Title: Language Models are Unsupervised Multitask Learners

Authors: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever

Published: Feb 14 2019

Link: [https://openai.com/index/better-language-models/](https://openai.com/index/better-language-models/)

Summary (Generated by Microsoft Copilot):

**Introduction:**
- The paper discusses how language models can learn various natural language processing tasks without explicit supervision by training on a large dataset called WebText.

**Challenges:**
- Current machine learning systems are brittle, sensitive to data distribution changes, and often require large, manually labeled datasets for each task.

**Methods:**
- The authors trained a large language model, GPT-2, on WebText, a dataset of millions of webpages, to perform tasks in a zero-shot setting without task-specific training.

**Novelties:**
- Demonstrating that language models can perform tasks like question answering, translation, and summarization without explicit supervision.
- Using a diverse and large dataset to improve generalization across tasks.

**Results:**
- GPT-2 achieved state-of-the-art results on 7 out of 8 language modeling datasets in a zero-shot setting.

**Performances:**
- The model showed significant improvements in tasks such as reading comprehension, translation, and summarization.

**Limitations:**
- The model still underfits the WebText dataset and has room for improvement in performance on certain tasks.

**Discussion:**
- The findings suggest a promising path towards building more general language processing systems that can learn from naturally occurring demonstrations without the need for extensive labeled datasets.
