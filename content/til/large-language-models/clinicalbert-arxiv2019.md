---
title: 'Publicly Available Clinical BERT Embeddings'
description: ''
date: 2024-09-13T09:10:00+09:00
lastmod: 
draft: false
---

Title: Publicly Available Clinical BERT Embeddings

Authors: Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, Matthew B. A. McDermott

Published: Apr 6 2019

Link: [https://arxiv.org/abs/1904.03323](https://arxiv.org/abs/1904.03323)

Summary:

- MIT x Microsoft​
- 2 million notes in the MIMIC-III v1.4 ​
- database (Johnson et al., 2016)​

Summary (Generated by Microsoft Copilot):

**Introduction:**
- The paper introduces Clinical BERT models for clinical text, addressing the lack of publicly available pre-trained BERT models in this domain.

**Challenges:**
- General BERT models are not optimized for clinical narratives, which have unique linguistic characteristics.

**Methods:**
- Two BERT models were trained: one on all clinical notes and another specifically on discharge summaries using the MIMIC-III database.

**Novelties:**
- Release of domain-specific BERT models for clinical text, demonstrating improvements over general BERT and BioBERT.

**Results:**
- Clinical BERT models showed performance improvements on three clinical NLP tasks but not on de-identification tasks.

**Performances:**
- Achieved state-of-the-art accuracy on MedNLI and improved performance on i2b2 2010 and 2012 tasks.

**Limitations:**
- The models did not improve de-identification tasks due to differences in text distribution.

**Discussion:**
- The study highlights the benefits of domain-specific embeddings and suggests further research with more advanced models and diverse datasets.
