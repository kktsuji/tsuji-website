---
title: 'PFPs: Prompt-guided Flexible Pathological Segmentation for Diverse Potential Outcomes Using Large Vision and Language Models'
description: ''
date: 2024-08-08T09:00:00+09:00
lastmod: 
draft: false
---

Title: PFPs: Prompt-guided Flexible Pathological Segmentation for Diverse Potential Outcomes Using Large Vision and Language Models

Authors: Can Cui, Ruining Deng, Junlin Guo, Quan Liu, Tianyuan Yao, Haichun Yang, Yuankai Huo

Published: Jul 13 2024

Link: [https://arxiv.org/abs/2407.09979](https://arxiv.org/abs/2407.09979)

Summary:

- Authors proposed a method called PFPs that increases a potential and flexibility of the efficient segment anything model (EfficientSAM, Xiong et al., 2024) for pathology image segmentation tasks.
- They was inspired by Omni-seg (Deng et al., 2023) and HATs (Deng et al., 2024).
- Low-rank adaptation (LoRA, Hu et al., 2021) was used for fine-tuning of pre-trained large language model (LLM) called TinyLLaMA (Zhang et al., 2024).
- Dataset: a kidney dataset NEPTUNE (Barisoni et al., 2013).
- They define 9 types of tasks such as "Segmentation of the nuclei outside the capsule region".
- What I learned: Segment anything model (SAM, Kirillov, 2023), dynamic head concept in Omni-seg and HATs.

Summary (Generated by Microsoft Copilot):

**Introduction:**
- The paper explores the use of Vision Foundation Models and Large Language Models (LLMs) for flexible pathological image segmentation.

**Challenges:**
- Current models lack flexibility and precision in segmenting diverse and complex structures in pathology images.

**Methods:**
- The proposed method integrates language prompts with spatial annotations using EfficientSAM and TinyLlama-1.1B models.

**Novelties:**
- Introduction of a computational-efficient pipeline using fine-tuned language prompts for multi-class segmentation.

**Results:**
- The approach shows improved flexibility and accuracy in segmenting kidney pathology images.

**Performances:**
- The model's performance is evaluated using Dice scores, showing better results with complete training sets.

**Limitations:**
- Limited data and computational resources restrict large-scale experiments.

**Discussion:**
- Future research aims to incorporate more diverse language prompts and larger datasets for better generalization.
