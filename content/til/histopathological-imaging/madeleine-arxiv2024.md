---
title: 'Multistain Pretraining for Slide Representation Learning in Pathology'
description: ''
date: 2024-08-10T09:30:00+09:00
lastmod: 
draft: false
---

Title: Multistain Pretraining for Slide Representation Learning in Pathology

Authors: Guillaume Jaume, Anurag Vaidya, Andrew Zhang, Andrew H. Song, Richard J. Chen, Sharifa Sahai, Dandan Mo, Emilio Madrigal, Long Phi Le, Faisal Mahmood

Published: Aug 5 2024

Link: [https://arxiv.org/abs/2408.02859](https://arxiv.org/abs/2408.02859)

Summary:

- Authors proposed a multimodal pretraining strategy called "MADELEINE", a slide representation learning method, leveraging multiple stained slides images for richer training.
- Trained MADELEINE encoder can be used for some downstream tasks such as few-shot classification, prognostication and fine-tuning.
- Training process utilizes a dual global-local cross-stain alignment objective with breast cancer samples and kidney transplant samples.

Summary (Generated by Microsoft Copilot):

**Introduction:**
- The paper introduces **Madeleine**, a multimodal pretraining strategy for slide representation learning in computational pathology.

**Challenges:**
- Existing methods are limited by the **clinical and biological diversity** of views and the **scale of whole-slide images (WSIs)**.

**Methods:**
- Madeleine uses a **dual global-local cross-stain alignment objective** on large cohorts of breast and kidney samples.

**Novelties:**
- Introduces a **multimodal pretraining strategy** leveraging multiple stains as different views to form a rich training signal.

**Results:**
- Demonstrates the quality of slide representations on various downstream evaluations, including **morphological and molecular classification**.

**Performances:**
- Madeleine outperforms existing methods in **few-shot classification** and **full classification** tasks.

**Limitations:**
- The study does not involve datasets used for downstream tasks, precluding any data leakage.

**Discussion:**
- Highlights the importance of using clinically and biologically meaningful views provided by multimodal pretraining.
