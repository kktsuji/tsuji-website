---
title: 'ReLU: Rectified Linear Unit'
description: ''
date: 2024-12-04T18:00:00+09:00
lastmod: 
draft: false
math: true
---

$ ReLU(x) = max(0, x)$

where $ x $ denotes the input to the neuron.

Variants:

- Leaky ReLU
- Parametric ReLU
- Constructed ReLU
- Gaussian-error Linear Unit (GeLU)
- etc.

References:

- [Rectifier (neural networks) - wiki](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
