---
title: "ReLU: Rectified Linear Unit"
description: ""
date: 2024-12-04T18:00:00+09:00
lastmod:
draft: false
math: true
---

$ ReLU(x) = max(0, x)$

where $ x $ denotes the input to the neuron.

Variants:

- Leaky ReLU
- Parametric ReLU
- Constructed ReLU
- Gaussian-error Linear Unit (GeLU)
- etc.

References:

- [Rectifier (neural networks) - wiki](<https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>)
